{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[Выборка](https://github.com/natasha/nerus)**: набор предложений на русском языке с указанием частей речи для каждого слова.\n",
    "\n",
    "Требуется:\n",
    "1. Рассмотреть последовательность частей речи как марковскую модель. Определить оптимальный порядок марковской модели.\n",
    "2. Обучить скрытую марковскую модель по выборке. Оценить точность предсказания частей речи, посчитать энтропию на выборке.\n",
    "\n",
    "**Важно**: в целях ускорения эксперимента рекомендуется взять первые 10 МБ текста из выборки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk.lm as lm\n",
    "from nltk.util import ngrams as nltk_ngrams\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = 0\n",
      "# sent_id = 0_0\n",
      "# text = Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее высокая смертность от рака, сообщает РИА Новости.\n",
      "1\tВице-премьер\t_\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t7\tnsubj\t_\tTag=O\n",
      "2\tпо\t_\tADP\t_\t_\t4\tcase\t_\tTag=O\n",
      "3\tсоциальным\t_\tADJ\t_\tCase=Dat|Degree=Pos|Number=Plur\t4\tamod\t_\tTag=O\n",
      "4\tвопросам\t_\tNOUN\t_\tAnimacy=Inan|Case=Dat|Gender=Masc|Number=Plur\t1\tnmod\t_\tTag=O\n",
      "5\tТатьяна\t_\tPROPN\t_\tAnimacy=Anim|Case=Nom|Gender=Fem|Number=Sing\t1\tappos\t_\tTag=B-PER\n",
      "6\tГоликова\t_\tPROPN\t_\tAnimacy=Anim|Case=Nom|Gender=Fem|Number=Sing\t5\tflat:name\t_\tTag=I-PER\n",
      "7\tрассказала\t_\tVERB\t_\tAspect=Perf|Gender=Fem|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\t0\troot\t_\tTag=O\n",
      "\n",
      "gzip: stdout: Broken pipe\n",
      "data/nerus_lenta.conllu\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c data/nerus_lenta.conllu.gz | head\n",
    "!ls data/nerus_lenta.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerus import load_nerus\n",
    "\n",
    "docs = load_nerus(\"data/nerus_lenta.conllu.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [00:45, 219.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "list_of_sent = []\n",
    "list_of_toks = []\n",
    "\n",
    "for i, doc in tqdm.tqdm(enumerate(docs)):\n",
    "    for sent in doc.sents:\n",
    "        list_of_toks_in_sent = []\n",
    "        list_of_tags_in_sent = []\n",
    "        for tok in sent.tokens:\n",
    "            list_of_toks_in_sent.append(tok.text)\n",
    "            list_of_tags_in_sent.append(tok.pos)\n",
    "        list_of_sent.append(list_of_toks_in_sent)\n",
    "        list_of_toks.append(list_of_tags_in_sent)\n",
    "    if i > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определить оптимальный порядок марковской модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим начальные и конечные токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_toks_padded = [['<s>']+i+['<\\s>'] for i in list_of_toks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cловарь n-грамм и их частот и n-граммных префиксов и их частот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def ngrams_and_prefix_counts(s_tokens, n_max):\n",
    "    ngrams_counts = {}\n",
    "    prefix_counts = {}\n",
    "    \n",
    "    for i in range(n_max):\n",
    "        ngrams_counts[i + 1] = Counter()\n",
    "        prefix_counts[i + 1] = Counter()\n",
    "        \n",
    "    for sentence in s_tokens:\n",
    "        n = len(sentence)\n",
    "        for i in range(n_max):\n",
    "            ngrams_counts[i + 1] += Counter([tuple(sentence[j : j + i + 1]) for j in range(n - i)])\n",
    "            prefix_counts[i + 1] += Counter([tuple(sentence[j : j + i] + ['*']) for j in range(n - i)])\n",
    "\n",
    "    return ngrams_counts, prefix_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_counts, prefix_counts = ngrams_and_prefix_counts(list_of_toks_padded, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-граммы и их частотные вероятности\n",
    "\n",
    "$$\\hat p_i = \\hat p(w_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probas(ngram_counts, prefix_counts):\n",
    "    p1 = {}\n",
    "    for w in ngram_counts[2]:\n",
    "        pre_w = tuple([w[0]] + ['*'])\n",
    "        p1[u'{0}'.format(*w)] = ngram_counts[2][w] / prefix_counts[2][pre_w]\n",
    "    return p1\n",
    "\n",
    "def bigram_probas(ngram_counts, prefix_counts):\n",
    "    p2 = {}\n",
    "    for w in ngram_counts[2]:\n",
    "        pre_w = tuple([w[0]] + ['*'])\n",
    "        p2[u'{1}|{0}'.format(*w)] = ngram_counts[2][w] / prefix_counts[2][pre_w]\n",
    "    return p2\n",
    "\n",
    "def trigram_probas(ngram_counts, prefix_counts):\n",
    "    p3 = {}\n",
    "    for w in ngram_counts[3]:\n",
    "        pre_w = w[:2] + tuple(['*'])\n",
    "        p3[u'{2}|{1},{0}'.format(*w)] = ngram_counts[3][w] / prefix_counts[3][pre_w]\n",
    "    return p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = unigram_probas(ngrams_counts, prefix_counts)\n",
    "p2 = bigram_probas(ngrams_counts, prefix_counts)\n",
    "p3 = trigram_probas(ngrams_counts, prefix_counts)\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверка гипотезы, что триграммную модель можно свести к биграммной против правосторонней альтернативы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистика:\n",
    "$$-2 \\log (\\prod_{i, j, k = 1}^m (\\hat p_{ij} / \\hat p_{ijk})^{n_{ijk}}) = \\sum_{i, j, k}^m -2 n_{ijk} \\log \\hat p_{ij} + 2 n_{ijk} \\log \\hat p_{ijk} = \\sum_{i = 3}^N -2 \\log \\hat p_{i,i - 1} + 2 \\log \\hat p_{i, i - 1, i - 2},$$\n",
    "$$n_{ijk} = |\\{X_t: X_t = O_i, X_{t + 1} = O_j, X_{t + 2} = O_k\\}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_statistic(p2, p3, tokens):\n",
    "    stat2 = []\n",
    "    stat3 = []\n",
    "    for sent in tokens:\n",
    "        n = len(sent)\n",
    "        for i in range(n - 2):\n",
    "            w = sent[i : i + 3]\n",
    "            ngram3 = '{2}|{1},{0}'.format(*w)\n",
    "            ngram2 = '{1}|{0}'.format(*w)\n",
    "            stat2.append(np.clip(np.log(p2[ngram2]), np.log(1e-20), np.log(1e+20)))\n",
    "            stat3.append(np.clip(np.log(p3[ngram3]), np.log(1e-20), np.log(1e+20)))\n",
    "    return 2 * np.sum(stat3) - 2 * np.sum(stat2) \n",
    "\n",
    "def chi2_statistic_l(p1, p2, tokens):\n",
    "    stat1 = []\n",
    "    stat2 = []\n",
    "    for sent in tokens:\n",
    "        n = len(sent)\n",
    "        for i in range(n - 2):\n",
    "            w = sent[i : i + 2]\n",
    "            ngram2 = '{1}|{0}'.format(*w)\n",
    "            ngram1 = '{1}'.format(*w)\n",
    "            stat2.append(np.clip(np.log(p2[ngram2]), np.log(1e-20), np.log(1e+20)))\n",
    "            stat1.append(np.clip(np.log(p1[ngram1]), np.log(1e-20), np.log(1e+20)))\n",
    "    return 2 * np.sum(stat2) - 2 * np.sum(stat1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value = 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "m = len(p3)\n",
    "stat = chi2_statistic(p2, p3, list_of_toks_padded)\n",
    "\n",
    "print(f'p-value = {1 - st.distributions.chi2(m * ((m - 1) ** 2) - 1).cdf(stat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значит триграммная модель бесполезна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value = 0.0\n"
     ]
    }
   ],
   "source": [
    "m = len(p2)\n",
    "stat = chi2_statistic_l(p1, p2, list_of_toks_padded)\n",
    "\n",
    "print(f'p-value = {1 - st.distributions.chi2(m * ((m - 1) ** 2) - 1).cdf(stat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**А вот уже с биграмной так сделать нельзя — она полезна!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соответственно, оптимальный порядок марковской модели = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что нам оно выдаст ([source](https://stackoverflow.com/questions/54959340/nltk-language-modeling-confusion))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'VERB']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.lm as lm\n",
    "\n",
    "mle = lm.MLE(2)\n",
    "train, tr_vocab = lm.preprocessing.padded_everygram_pipeline(2, list_of_toks)\n",
    "mle.fit(train, tr_vocab)\n",
    "mle.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вроде бы даже что-то разумное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценить точность предсказания частей речи, посчитать энтропию на выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(list_of_toks)):\n",
    "    for j in range(len(list_of_sent[i]))\n",
    "        data.append[(list_of_sent[i][j], list_of_toks[i][j])]\n",
    "    \n",
    "data_train = data[ int(len(data)*0.1):]\n",
    "data_test  = data[:int(len(data)*0.1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11929/11929 [13:48<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9484459872079195\n",
      "Mean entropy = 6.539898218874228\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import hmm\n",
    "\n",
    "tagger = hmm.HiddenMarkovModelTagger.train(data_train)\n",
    "mean_entropy = 0  \n",
    "\n",
    "for i in tqdm.tqdm(data_test):\n",
    "    entropy = tagger.entropy(i)\n",
    "    if entropy < np.inf:\n",
    "        mean_entropy += entropy\n",
    "        \n",
    "mean_entropy /= len(data_test) \n",
    "\n",
    "print(f'Accuracy = {tagger.evaluate(data_test)}')\n",
    "print(f'Mean entropy = {mean_entropy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
